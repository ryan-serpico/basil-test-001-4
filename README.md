# test-001 - Semantic Document Search

This repository contains a semantic search application for the "test-001" document collection, generated by [Basil](https://github.com/ryan-serpico/basil).

## 📋 Overview

- **Project**: test-001
- **Reporter**: Ryan Serpico
- **Publication**: San Antonio Express News
- **Generated**: September 26, 2025 at 09:44 PM UTC
- **Documents**: 3 files processed
- **Chunks**: 1287 text segments
- **Tokens**: 128593 processed

## 🚀 Quick Start

### Prerequisites

- [Docker](https://www.docker.com/get-started) installed on your computer
- Basic familiarity with command line/terminal

### Running the Application

1. **Download or clone this repository** to your computer

2. **Open terminal/command prompt** and navigate to this directory:
   ```bash
   cd test-001
   ```

3. **Start the application** using Docker Compose:
   ```bash
   docker compose up --build
   ```

4. **Open your web browser** and visit:
   ```
   http://localhost:8000
   ```

5. **Start searching!** Enter natural language queries to search through your documents.

### Stopping the Application

To stop the application, press `Ctrl+C` in the terminal, or run:
```bash
docker compose down
```

## 🔍 How to Search

This application uses **semantic search**, which means you can search using natural language rather than exact keywords.

### Example Queries:
- "What are the main findings about climate change?"
- "Tell me about budget discussions"
- "Any mentions of public safety concerns?"
- "Information about educational policies"

### Search Features:
- **Semantic Understanding**: Finds relevant content even if exact words don't match
- **Context Display**: Shows surrounding text for better understanding
- **Source Information**: Identifies which document contains each result
- **Export Options**: Download results as JSON or CSV files

## 📁 Repository Structure

```
test-001/
├── README.md                 # This file
├── docker-compose.yml        # Docker configuration
├── Dockerfile               # Container build instructions
├── data/                    # Document data
│   ├── documents/          # Original uploaded files
│   ├── chunks.json         # Processed text segments
│   └── chroma_db/          # Vector database
├── backend/                 # Search API (Python/FastAPI)
│   ├── main.py             # Main application
│   ├── vector_db.py        # Database service
│   └── requirements.txt    # Python dependencies
└── frontend/               # Web interface (React)
    ├── src/               # Source code
    ├── public/            # Static files
    └── package.json       # Node.js dependencies
```

## 🛠 Advanced Usage

### Development Mode

If you want to modify the application, you can run it in development mode:

```bash
# Run backend only (API will be available at http://localhost:8000)
cd backend
python -m uvicorn main:app --reload

# In another terminal, run frontend (will be available at http://localhost:3000)
cd frontend
npm install
npm start
```

### Custom Configuration

The application can be customized by modifying:
- **Search parameters**: Edit `backend/vector_db.py`
- **UI appearance**: Modify `frontend/src/App.css`
- **API endpoints**: Update `backend/main.py`

### Data Export

Search results can be exported in three formats:
1. **JSON**: Structured data with full metadata
2. **CSV**: Spreadsheet-compatible format
3. **Parquet**: High-performance columnar format with full embeddings

Use the export buttons in the web interface after performing a search.

## 🔧 Technical Details

### Technologies Used:
- **Backend**: Python, FastAPI, Polars, NumPy
- **Frontend**: React, JavaScript, CSS
- **Vector Storage**: Parquet files with Polars DataFrame processing
- **Search Engine**: NumPy dot product similarity (following Max Woolf's approach)
- **Deployment**: Single Docker container (simplified architecture)

### Search Process:
1. Your query is converted to a vector embedding (using OpenAI text-embedding-3-large)
2. NumPy performs fast dot product calculations against the embeddings matrix
3. Results are ranked by cosine similarity and returned with context
4. Polars handles efficient data filtering and metadata retrieval

### Performance Benefits:
- **Vector Dimensions**: 3072 (OpenAI text-embedding-3-large)
- **Chunk Size**: 100 tokens with 50 token overlap
- **Search Speed**: ~1ms similarity search for typical newsroom collections (32k chunks)
- **Memory Efficiency**: Zero-copy operations with Polars
- **Portability**: Self-contained Parquet files, no external database required
- **Startup Time**: Faster initialization compared to traditional vector databases

## 📊 Collection Statistics

- **Total Documents**: 3
- **Total Text Chunks**: 1287
- **Total Tokens Processed**: 128593
- **Average Document Size**: 0.0
- **Processing Date**: September 26, 2025 at 09:44 PM UTC

### Document List:
- **2021_2118_Nicholas_E_Jegabbi_et_v_Nicholas_E_Jegabbi_et__SUMMONS___COMPLAINT_1OCR.md**: 514 chunks, 51358 tokens (0 MB)
- **2021_2249_EDWARD_WILDZUNAS_et_al_v_EDWARD_WILDZUNAS_et_al_SUMMONS___COMPLAINT_1OCR.md**: 244 chunks, 24361 tokens (0 MB)
- **2023_5531_Christopher_Van_Zile_e_v_Christopher_Van_Zile_e_SUMMONS___COMPLAINT_1OCR.md**: 529 chunks, 52874 tokens (0 MB)

## ❗ Troubleshooting

### Application Won't Start
- Ensure Docker is running
- Check that port 8000 is not in use by another application
- Try running `docker compose down` then `docker compose up --build`

### No Search Results
- Try broader or more specific search terms
- Check that the vector database loaded successfully (see application logs)
- Verify data files are present in the `data/` directory

### Slow Performance
- The first search after startup may be slower due to initialization
- Large document collections may require more time to process queries
- Consider running on a machine with more RAM for better performance

### Getting Help
If you encounter issues:
1. Check the application logs: `docker compose logs`
2. Ensure you have the latest version of Docker
3. Try rebuilding the containers: `docker compose up --build --force-recreate`

## 📝 Generated by Basil

This search application was automatically generated by **Basil**, a tool created by San Antonio Express News to democratize semantic search for newsrooms.

### About Basil:
- Converts document collections into searchable repositories
- Uses state-of-the-art AI embeddings for semantic understanding
- Creates self-contained, deployable applications
- Designed for non-technical users

**Learn more**: [Basil GitHub Repository](https://github.com/ryan-serpico/basil)

---

*Last updated: September 26, 2025 at 09:44 PM UTC*